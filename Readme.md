# ğŸ¤– Dual Gemma Model Chat

An interactive user interface for automatic conversation between two Gemma language models running locally.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)
![Ollama](https://img.shields.io/badge/Ollama-Latest-green.svg)

---

## ğŸ“‹ Table of Contents

- [Introduction](#-introduction)
- [Features](#-features)
- [Prerequisites](#-prerequisites)
- [Installation & Setup](#-installation--setup)
- [Usage Guide](#-usage-guide)
- [Project Structure](#-project-structure)
- [Screenshots](#-screenshots)
- [Technical Architecture](#-technical-architecture)
- [Troubleshooting](#-troubleshooting)
- [Advanced Settings](#-advanced-settings)

---

## ğŸ¯ Introduction

This project was developed as part of a job interview task and enables **automatic dialogue** between two Gemma language models. Users can define a **Role** and **System Prompt** for each model and observe natural, intelligent interaction between them.

### ğŸ“ Use Cases:
- Simulating educational conversations (teacher-student)
- Testing and comparing different model behaviors
- Generating creative content (storytelling, debates)
- Research in Multi-Agent Dialogue Systems

---

## âœ¨ Features

- âœ… **Fully Local Execution**: No internet, API keys, or external services required
- âœ… **Gemma Model Support**: Gemma 3 (4B), Gemma 3n (4B)
- âœ… **Role & Prompt Configuration**: Complete control over each model's behavior
- âœ… **Persian UI**: Designed for Persian-speaking users
- âœ… **Real-time Display**: Live conversation view with distinct color coding
- âœ… **Context Management**: Intelligent conversation history maintenance
- âœ… **Flow Control**: Start, Stop, and Clear buttons
- âœ… **Save Conversations**: Download history as `.txt` file
- âœ… **Progress Tracking**: Visual progress bar during conversation
- âœ… **Stats & Reports**: Display message count and response times

---

## ğŸ“¦ Prerequisites

### Recommended Hardware:
```
ğŸ’¾ RAM: Minimum 8GB (16GB for optimal performance)
ğŸ’¿ Disk Space: 5-10GB for models
ğŸ–¥ï¸ CPU: Modern processor (GPU optional for faster inference)
```

### Software:
- **Python**: Version 3.8 or higher
- **Ollama**: For running Gemma models locally
- **Python Packages**: Streamlit and Ollama

---

## ğŸš€ Installation & Setup

### Step 1ï¸âƒ£: Install Ollama

#### Windows:
1. Visit [ollama.com/download](https://ollama.com/download)
2. Download and run the Windows installer
3. Ollama will install and start automatically

#### Linux/Mac:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Test Installation:**
```bash
ollama --version
```

---

### Step 2ï¸âƒ£: Download Gemma Models

```bash
# Start Ollama service (runs in background)
ollama serve

# Download models (in a new terminal)
ollama run gemma3n
ollama run gemma3n:e4b

# Check installed models
ollama list
```

**Expected Output:**
```
NAME            ID              SIZE    MODIFIED
gemma3:4b       abc123def456    3.3GB   2 minutes ago
gemma3n:e4b     def789ghi012    7.5GB   5 minutes ago
```

---

### Step 3ï¸âƒ£: Install Python Packages

```bash
# Create virtual environment (recommended)
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

**Contents of `requirements.txt`:**
```txt
streamlit>=1.28.0
ollama>=0.1.0
```

---

### Step 4ï¸âƒ£: Test System

```bash
# Run test script
python test_script.py
```

This script checks:
- âœ… Python version
- âœ… Ollama installation
- âœ… Ollama service status
- âœ… Installed models
- âœ… Model inference (functionality test)
- âœ… Streamlit installation
- âœ… Disk space

---

### Step 5ï¸âƒ£: Run Application

```bash
streamlit run app.py
```

The application will run at:
```
ğŸŒ Local URL: http://localhost:8501
ğŸ”— Network URL: http://192.168.x.x:8501
```

---

## ğŸ“– Usage Guide

### 1. Initial Settings (Sidebar)

#### ğŸ”µ Model 1:
- **Select Model**: `gemma3:4b`, `gemma3n:e4b`
- **Model Role**: e.g., "Physics Teacher"
- **System Prompt**: 
  ```
  You are a patient and precise physics teacher who explains 
  concepts in simple language. Give short and helpful answers.
  ```

#### ğŸŸ¢ Model 2:
- **Select Model**: Choose a different model for variety
- **Model Role**: e.g., "Curious Student"
- **System Prompt**:
  ```
  You are a curious student who asks many questions and wants 
  to learn more. Ask short and relevant questions.
  ```

#### âš™ï¸ Conversation Settings:
- **Number of Turns**: 2 to 15 (default: 8)
- **Initial Message**: Message that starts the conversation

---

### 2. Start Conversation

1. Click **"â–¶ï¸ Start Conversation"** button
2. Model 1 receives initial message and responds
3. Model 2 receives Model 1's response and replies
4. Process continues for the specified number of turns

---

### 3. Control Conversation

- **â¸ï¸ Stop**: Pause conversation at any time
- **ğŸ”„ View Conversation**: Refresh page to view history
- **ğŸ—‘ï¸ Clear**: Delete all history and start fresh

---

### 4. Save & Export

After conversation ends:
- Click **"ğŸ’¾ Save Conversation"**
- `.txt` file containing full conversation will be downloaded

**Output File Format:**
```
Conversation between Physics Teacher and Curious Student
Date: 2025-11-21 21:53:12
==================================================

Physics Teacher (21:53:15):
Gravity is one of the four fundamental forces in nature...

Curious Student (21:53:20):
Why do all objects fall at the same speed?
...
```

---

## ğŸ“‚ Project Structure

```
PROJECT/
â”‚
â”œâ”€â”€ ğŸ“ fonts/                          # Project fonts
â”‚   â””â”€â”€ Dana-Regular.ttf
â”‚
â”œâ”€â”€ ğŸ“ icons/                          # SVG icons
â”‚   â”œâ”€â”€ checkmark.svg
â”‚   â”œâ”€â”€ clear.svg
â”‚   â”œâ”€â”€ download.svg
â”‚   â”œâ”€â”€ model.svg
â”‚   â”œâ”€â”€ pause.svg
â”‚   â”œâ”€â”€ play.svg
â”‚   â”œâ”€â”€ play1.svg
â”‚   â”œâ”€â”€ refresh.svg
â”‚   â”œâ”€â”€ robot.svg
â”‚   â”œâ”€â”€ save.svg
â”‚   â”œâ”€â”€ setting.svg
â”‚   â””â”€â”€ trash.svg
â”‚
â”œâ”€â”€ ğŸ“ result/                         # Results and outputs
â”‚   â”œâ”€â”€ conversation_20251121_021235.txt
â”‚   â”œâ”€â”€ conversation_20251121_021649.txt
â”‚   â”œâ”€â”€ Screenshot 2025-11-21 021420.png
â”‚   â”œâ”€â”€ Screenshot 2025-11-21 021449.png
â”‚   â”œâ”€â”€ Screenshot 2025-11-21 021503.png
â”‚   â”œâ”€â”€ Screenshot 2025-11-21 021551.png
â”‚   â”œâ”€â”€ Screenshot 2025-11-21 021809.png
â”‚   â”œâ”€â”€ Screenshot 2025-11-21 022145.png
â”‚   â”œâ”€â”€ Screenshot 2025-11-21 024324.png
â”‚   â”œâ”€â”€ Screenshot 2025-11-21 111606.png
â”‚   â””â”€â”€ streamlit-app-2025-11-21-02-11-44.webm
â”‚
â”œâ”€â”€ ğŸ“„ app.py                          # Main Streamlit file
â”œâ”€â”€ ğŸ“„ requirements.txt                # Python dependencies
â”œâ”€â”€ ğŸ“„ test_script.py                  # System test script
â””â”€â”€ ğŸ“„ README.md                       # This file
```

---

## ğŸ“¸ Screenshots

Sample screenshots from the UI and results are available in the `result/` folder:

### Main UI:
![Main UI](result/Screenshot%202025-11-21%20111606.png)

### Conversation in Progress:
![Conversation](result/Screenshot%202025-11-21%20024324.png)

### Settings Panel:
![Settings](result/Screenshot%2025-11-21%021503.png)
![Settings](result/Screenshot%2025-11-21%021551.png)


**Demo Video:**
- `result/streamlit-app-2025-11-21-02-11-44.webm`

---

## ğŸ—ï¸ Technical Architecture

### Workflow:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    User     â”‚
â”‚  User Input â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit Frontend    â”‚
â”‚  - UI Components        â”‚
â”‚  - Session State        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Conversation Loop     â”‚
â”‚  - Turn Management      â”‚
â”‚  - Context Building     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚          â”‚
       â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model 1  â”‚  â”‚ Model 2  â”‚
â”‚ (Gemma)  â”‚  â”‚ (Gemma)  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Ollama Server â”‚
    â”‚  (localhost)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Conversation Mechanism:

```python
# Turn 1
Message_0 = "Hello! Let's talk about gravity."
Response_1 = Model_1(system_prompt_1 + Message_0)

# Turn 2
Response_2 = Model_2(system_prompt_2 + Response_1 + History)

# Turn 3
Response_3 = Model_1(system_prompt_1 + Response_2 + History)

# ... continues for N turns
```

### Context Management:

To prevent prompt length issues and maintain coherence:
- **Last 4 messages** are provided as context to the model
- Complete history is stored in `st.session_state`
- Each message includes: `speaker`, `message`, `model`, `timestamp`

---

## ğŸ”§ Troubleshooting

### âŒ "Connection Refused" Error

**Cause:** Ollama service is not running

**Solution:**
```bash
# Start Ollama
ollama serve
```

---

### âŒ "Model not found" Error

**Cause:** Model has not been downloaded

**Solution:**
```bash
# Check available models
ollama list

# Download required model
ollama pull gemma3:4b
ollama pull gemma3n:e4b

```

---

### âš ï¸ Very Slow Performance

**Cause:** Using CPU for inference

**Solutions:**
1. Use smaller models (`gemma2:2b`)
2. Reduce `num_predict` (in `app.py`)
3. Decrease number of turns
4. Use GPU if available

```python
# In app.py, line ~220
options={
    'temperature': 0.8,
    'num_predict': 100,  # Reduced from 150 to 100
}
```

---

### âš ï¸ Repetitive Responses

**Solutions:**
1. Increase `temperature`:
```python
'temperature': 0.9  # Instead of 0.8
```

2. Write more specific system prompts
3. Use different models for each role

---

### âŒ "Out of Memory" Error

**Solutions:**
- Use the same model for both roles instead of two different ones
- Choose a smaller model
- Close other applications

---

## âš™ï¸ Advanced Settings

### Adjusting Model Parameters:

In `app.py`, function `call_model`:

```python
response = ollama.generate(
    model=model_name,
    prompt=full_prompt,
    options={
        'temperature': 0.8,      # 0.0 (deterministic) to 1.0 (creative)
        'num_predict': 150,      # Maximum response tokens
        'top_p': 0.9,           # Nucleus sampling
        'top_k': 40,            # Number of candidate tokens
        'repeat_penalty': 1.1,  # Repetition penalty
    }
)
```

### Adding New Models:

```bash
# Download other models
ollama pull llama2:7b
ollama pull mistral:7b
ollama pull codellama:7b
```

Then add to model list in `app.py`:

```python
model1_name = st.selectbox(
    "Select Model:",
    ["gemma3:4b","gemma3n:e4b" ,"gemma2:9b", "gemma2:2b", 
     "llama2:7b", "mistral:7b"],  # Added
    key="model1_select"
)
```

---

## ğŸ“Š Model Comparison

| Model | Size | Speed (CPU) | Response Quality | RAM Required |
|-------|------|-------------|------------------|--------------|
| **gemma2:2b** | 1.5GB | â­â­â­â­â­ Fast | â­â­â­ Good | 4GB |
| **gemma3:4b** | 3.3GB | â­â­â­â­ Fast | â­â­â­â­ Excellent | 9GB |
| **gemma3n:e4b** | 7.5GB | â­â­â­ Medium | â­â­â­â­â­ Outstanding | 13GB |

**Recommendations:**
- For quick testing: `gemma2:2b`
- For speed/quality balance: `gemma3:4b`
- For best quality: `gemma3n:e4b`

---

## ğŸ¯ Usage Scenarios

### 1. Education:
```
Role 1: Math Teacher
Role 2: Student
Topic: Teaching mathematical concepts
```

### 2. Debate:
```
Role 1: Technology Advocate
Role 2: Technology Critic
Topic: Impact of AI
```

### 3. Storytelling:
```
Role 1: Story Narrator
Role 2: Main Character
Topic: A sci-fi adventure
```

### 4. Job Interview:
```
Role 1: Interviewer
Role 2: Candidate
Topic: Programming interview
```

---

## ğŸ“ Developer Notes

### Development Time:
- Design & Programming: **6 hours**
- Testing & Debugging: **3 hours**
- Documentation: **2 hours**

### Main Challenges:
1. âœ… Memory management for two simultaneous models
2. âœ… Preventing repetitive loops
3. âœ… Designing responsive UI with Streamlit
4. âœ… Context management for coherence

### Future Improvements:
- [ ] Support for multiple models (3+)
- [ ] Export to JSON/PDF
- [ ] Display metrics (tokens/sec, latency)
- [ ] Multi-language support
- [ ] Langchain integration

---

## ğŸ“š Resources

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Streamlit Documentation](https://docs.streamlit.io)
- [Gemma Models](https://ai.google.dev/gemma)
- [Python Ollama Library](https://github.com/ollama/ollama-python)

---

## ğŸ“§ Contact & Support

For questions and support:
- **Developer**: [Zahra Bahadori]
- **Email**: Zbahadori107@gmail.com

---

## ğŸ™ Acknowledgments

- **Ollama Team** for the excellent local model execution tool
- **Streamlit** for the simple yet powerful framework
- **Google DeepMind** for open-source Gemma models

---

## ğŸ“œ License

This project is free for personal and educational use.

---

<div align="center">

**Built for Job Interview Task**

`Version 1.0.0 | November 2025`

</div>
