"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØªØ³Øª Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ollama Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§
"""

import sys
import subprocess

def print_header(text):
    """Ú†Ø§Ù¾ Ù‡Ø¯Ø± Ø²ÛŒØ¨Ø§"""
    print("\n" + "="*60)
    print(f"  {text}")
    print("="*60 + "\n")

def check_python_version():
    """Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø³Ø®Ù‡ Python"""
    print_header("ğŸ Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø³Ø®Ù‡ Python")
    
    version = sys.version_info
    print(f"Python Version: {version.major}.{version.minor}.{version.micro}")
    
    if version.major >= 3 and version.minor >= 8:
        print("âœ… Ù†Ø³Ø®Ù‡ Python Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª")
        return True
    else:
        print("âŒ Python 3.8 ÛŒØ§ Ø¨Ø§Ù„Ø§ØªØ± Ù†ÛŒØ§Ø² Ø§Ø³Øª")
        return False

def check_ollama_installed():
    """Ø¨Ø±Ø±Ø³ÛŒ Ù†ØµØ¨ Ollama"""
    print_header("ğŸ¦™ Ø¨Ø±Ø±Ø³ÛŒ Ù†ØµØ¨ Ollama")
    
    try:
        result = subprocess.run(
            ['ollama', '--version'],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode == 0:
            print(f"âœ… Ollama Ù†ØµØ¨ Ø´Ø¯Ù‡ Ø§Ø³Øª")
            print(f"   Version: {result.stdout.strip()}")
            return True
        else:
            print("âŒ Ollama Ù†ØµØ¨ Ù†ÛŒØ³Øª ÛŒØ§ Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±Ø¯")
            return False
            
    except FileNotFoundError:
        print("âŒ Ollama ÛŒØ§ÙØª Ù†Ø´Ø¯")
        print("   Ù„Ø·ÙØ§Ù‹ Ø§Ø² https://ollama.com/download Ù†ØµØ¨ Ú©Ù†ÛŒØ¯")
        return False
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {e}")
        return False

def check_ollama_running():
    """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³ Ollama"""
    print_header("ğŸ”„ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø±ÙˆÛŒØ³ Ollama")
    
    try:
        result = subprocess.run(
            ['ollama', 'list'],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode == 0:
            print("âœ… Ø³Ø±ÙˆÛŒØ³ Ollama Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§ Ø§Ø³Øª")
            return True
        else:
            print("âŒ Ø³Ø±ÙˆÛŒØ³ Ollama Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§ Ù†ÛŒØ³Øª")
            print("   Ù„Ø·ÙØ§Ù‹ Ø¯Ø± ØªØ±Ù…ÛŒÙ†Ø§Ù„ Ø¬Ø¯ÛŒØ¯ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯: ollama serve")
            return False
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {e}")
        print("   Ø³Ø±ÙˆÛŒØ³ Ollama Ø±Ø§ Ø¨Ø§ 'ollama serve' Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯")
        return False

def check_models():
    """Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù†ØµØ¨ Ø´Ø¯Ù‡"""
    print_header("ğŸ“¦ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù†ØµØ¨ Ø´Ø¯Ù‡")
    
    try:
        result = subprocess.run(
            ['ollama', 'list'],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode != 0:
            print("âŒ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±Ø¯")
            return False
        
        output = result.stdout
        print("Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯:")
        print(output)
        
        required_models = ["gemma3:4b", "gemma3n:e4b"]
        found_models = []
        
        for model in required_models:
            if model in output:
                found_models.append(model)
                print(f"âœ… {model} ÛŒØ§ÙØª Ø´Ø¯")
            else:
                print(f"âŒ {model} ÛŒØ§ÙØª Ù†Ø´Ø¯")
        
        if len(found_models) >= 2:
            print(f"\nâœ… {len(found_models)} Ù…Ø¯Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø³Øª")
            return True
        else:
            print("\nâš ï¸  Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ùˆ Ù…Ø¯Ù„ Ù†ÛŒØ§Ø² Ø§Ø³Øª")
            print("   Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§:")
            print("   ollama pull gemma3:4b")
            print("   ollama pull gemma3n:e4b")
            return False
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {e}")
        return False

def test_model_inference():
    """ØªØ³Øª Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù…Ø¯Ù„"""
    print_header("ğŸ§ª ØªØ³Øª Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù…Ø¯Ù„")
    
    try:
        import ollama
        
        print("Ø¯Ø± Ø­Ø§Ù„ ØªØ³Øª Ù…Ø¯Ù„ gemma2:2b...")
        
        response = ollama.generate(
            model='gemma2:2b',
            prompt='Ø³Ù„Ø§Ù…! ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ú©ÙˆØªØ§Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ú¯Ùˆ.',
            options={'num_predict': 50}
        )
        
        print(f"âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø§Ø³Ø® Ø¯Ø§Ø¯:")
        print(f"   '{response['response'][:100]}...'")
        return True
        
    except ImportError:
        print("âŒ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ollama Ù†ØµØ¨ Ù†ÛŒØ³Øª")
        print("   Ù†ØµØ¨ Ú©Ù†ÛŒØ¯: pip install ollama")
        return False
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªÙ†ØªØ§Ø¬: {e}")
        return False

def check_streamlit():
    """Ø¨Ø±Ø±Ø³ÛŒ Ù†ØµØ¨ Streamlit"""
    print_header("ğŸ¨ Ø¨Ø±Ø±Ø³ÛŒ Streamlit")
    
    try:
        import streamlit
        print(f"âœ… Streamlit Ù†ØµØ¨ Ø´Ø¯Ù‡ Ø§Ø³Øª (v{streamlit.__version__})")
        return True
    except ImportError:
        print("âŒ Streamlit Ù†ØµØ¨ Ù†ÛŒØ³Øª")
        print("   Ù†ØµØ¨ Ú©Ù†ÛŒØ¯: pip install streamlit")
        return False

def check_disk_space():
    """Ø¨Ø±Ø±Ø³ÛŒ ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú©"""
    print_header("ğŸ’¾ Ø¨Ø±Ø±Ø³ÛŒ ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú©")
    
    try:
        import shutil
        
        total, used, free = shutil.disk_usage("/")
        
        free_gb = free // (2**30)
        print(f"ÙØ¶Ø§ÛŒ Ø¢Ø²Ø§Ø¯: {free_gb} GB")
        
        if free_gb > 5:
            print("âœ… ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú© Ú©Ø§ÙÛŒ Ø§Ø³Øª")
            return True
        else:
            print("âš ï¸  ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú© Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ø§ÙÛŒ Ù†Ø¨Ø§Ø´Ø¯")
            return False
            
    except Exception as e:
        print(f"âš ï¸  Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú© Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø±Ø¯: {e}")
        return True

def main():
    """Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù… ØªØ³Øªâ€ŒÙ‡Ø§"""
    print("\n" + "ğŸ” Ø´Ø±ÙˆØ¹ Ø¨Ø±Ø±Ø³ÛŒ Ø³ÛŒØ³ØªÙ…...")
    
    checks = {
        "Python Version": check_python_version(),
        "Ollama Installed": check_ollama_installed(),
        "Ollama Running": check_ollama_running(),
        "Models": check_models(),
        "Streamlit": check_streamlit(),
        "Disk Space": check_disk_space(),
    }
    
    # Ø§Ù…ØªØ­Ø§Ù† Ú©Ø±Ø¯Ù† inference (ÙÙ‚Ø· Ø§Ú¯Ù‡ Ø¨Ù‚ÛŒÙ‡ OK Ø¨Ø§Ø´Ù†Ø¯)
    if all([checks["Ollama Installed"], checks["Ollama Running"], checks["Models"]]):
        checks["Model Inference"] = test_model_inference()
    
    # Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
    print_header("ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬")
    
    for check_name, result in checks.items():
        status = "âœ…" if result else "âŒ"
        print(f"{status} {check_name}")
    
    print("\n" + "="*60)
    
    if all(checks.values()):
        print("\nğŸ‰ ØªÙ…Ø§Ù… ØªØ³Øªâ€ŒÙ‡Ø§ Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯! Ø´Ù…Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø³ØªÛŒØ¯.")
        print("   Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§: streamlit run app.py")
    else:
        print("\nâš ï¸  Ø¨Ø±Ø®ÛŒ Ù…Ø´Ú©Ù„Ø§Øª ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ø·Ø±Ù Ú©Ù†ÛŒØ¯.")
        print("\nØ±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø±ÙØ¹ Ù…Ø´Ú©Ù„Ø§Øª:")
        
        if not checks.get("Ollama Installed", True):
            print("  1. Ollama Ø±Ø§ Ø§Ø² https://ollama.com/download Ù†ØµØ¨ Ú©Ù†ÛŒØ¯")
        
        if not checks.get("Ollama Running", True):
            print("  2. Ollama Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯: ollama serve")
        
        if not checks.get("Models", True):
            print("  3. Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯:")
            print("     ollama pull gemma3:4b")
            print("     ollama pull gemma3n:e4b")
        
        if not checks.get("Streamlit", True):
            print("  4. Streamlit Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯:")
            print("     pip install streamlit ollama")
    
    print("="*60 + "\n")

if __name__ == "__main__":
    main()